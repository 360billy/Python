安装Celery


用pip或easy_install安装：

$ sudo pip install Celery

或着：

$ sudo easy_install Celery

使用Redis作为Broker时，再安装一个celery-with-redis。


开始编写tasks.py：
# tasks.py
import time
from celery import Celery

celery = Celery('tasks', broker='redis://localhost:6379/0')

@celery.task
def sendmail(mail):
    print('sending mail to %s...' % mail['to'])
    time.sleep(2.0)
    print('mail sent.')

然后启动Celery处理任务：

$ celery -A tasks worker --loglevel=info

上面的命令行实际上启动的是Worker，如果要放到后台运行，可以扔给supervisor。


如何发送任务？非常简单：

>>> from tasks import sendmail
>>> sendmail.delay(dict(to='celery@python.org'))
<AsyncResult: 1a0a9262-7858-4192-9981-b7bf0ea7483b>

可以看到，Celery的API设计真的非常简单。


然后，在Worker里就可以看到任务处理的消息：

[2013-08-27 19:20:23,363: WARNING/MainProcess] celery@MichaeliMac.local ready.
[2013-08-27 19:20:23,367: INFO/MainProcess] consumer: Connected to redis://localhost:6379/0.
[2013-08-27 19:20:45,618: INFO/MainProcess] Got task from broker: tasks.sendmail[1a0a9262-7858-4192-9981-b7bf0ea7483b]
[2013-08-27 19:20:45,655: WARNING/PoolWorker-4] sending mail to celery@python.org...
[2013-08-27 19:20:47,657: WARNING/PoolWorker-4] mail sent.
[2013-08-27 19:20:47,658: INFO/MainProcess] Task tasks.sendmail[1a0a9262-7858-4192-9981-b7bf0ea7483b] succeeded in 2.00266814232s: None

Celery默认设置就能满足基本要求。Worker以Pool模式启动，默认大小为CPU核心数量，缺省序列化机制是pickle，但可以指定为json。由于Python调用UNIX/Linux程序实在太容易，所以，用Celery作为异步任务框架非常合适。


Celery还有一些高级用法，比如把多个任务组合成一个原子任务等，还有一个完善的监控接口，以后有空再继续研究




新建一个task.py
from celery import Celery

celery = Celery('tasks', broker='amqp://')
celery.conf.CELERY_RESULT_BACKEND = os.environ.get('CELERY_RESULT_BACKEND', 'amqp')

@celery.task(name='task.db_operation')
def db_operation(id):
    # 耗时的数据库操作
    pass 
    
新建一个handler.py
import tcelery
tcelery.setup_nonblocking_producer()

from tasks import db_operation

calss Resource(RequestHandler):
    @asynchronous
    def get():
        # 参数通过args传递,回调通过callback指定
        db_operation.apply_async(args=[id], callback=self.on_success)
    def on_success(self, response):
        # 获取返回的结果
        resource = response.result
        self.write(resource)
        self.finish()
 
from concurrent.futures import ThreadPoolExecutor
from tornado.concurrent import run_on_executor

class ChatHandler(web.RequestHandler):
    executor = ThreadPoolExecutor(4)

    @web.asynchronous
    @gen.coroutine
    def get(self):
        resource = yield self.get_db_operation()
        self.write(resource)
        self.finish()

    @web.asynchronous
    @gen.coroutine
    def post(self):
        yield self.post_db_operation()
        self.write('success')
        self.finish()

    @run_on_executor
    def get_db_operation(self):
        return resource 

    @run_on_executor
    def post_db_operation(self):
        pass

